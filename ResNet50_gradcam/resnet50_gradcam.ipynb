{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "#!pip install torchcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from torchvision.transforms import Lambda\n",
    "from torchcam.utils import overlay_mask\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchcam.methods import GradCAM, GradCAMpp, SmoothGradCAMpp, ScoreCAM, SSCAM, XGradCAM, LayerCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltrasoundDataset(Dataset):\n",
    "    '''\n",
    "    Custom dataset class for ultrasound images\n",
    "\n",
    "    Args:\n",
    "    dataframe: pandas dataframe containing image paths and labels\n",
    "    transform: torchvision.transforms.Compose object for image augmentation for training/testing\n",
    "    transform_og: torchvision.transforms.Compose object for image augmentation for visualizing original images\n",
    "    return_og: bool, if True, returns original image along with transformed image\n",
    "\n",
    "    Returns:\n",
    "    image1: transformed image\n",
    "    label: label of the image\n",
    "    image2: original image (if return_og is True)\n",
    "    '''\n",
    "    def __init__(self, dataframe, transform=None, transform_og=None, return_og=False, return_img_name=False):   \n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.transform_og = transform_og\n",
    "        self.return_og = return_og\n",
    "        self.return_img_name = return_img_name\n",
    "\n",
    "        if self.return_og:\n",
    "            assert self.transform_og is not None, 'transform_og must be provided if return_og is True'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join('..',self.dataframe.iloc[idx, 0])\n",
    "        label = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "        if image.mode == 'RGBA':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform :\n",
    "            image1 = self.transform(image)\n",
    "        \n",
    "        # if self.return_og:  \n",
    "        #     image2 = self.transform_og(image)\n",
    "        #     return image1,label, image2\n",
    "        # else:\n",
    "        #     return image1, label\n",
    "        if self.return_og:\n",
    "           image2 = self.transform_og(image)\n",
    "           if self.return_img_name:\n",
    "               return image1, label, image2, img_name\n",
    "           else:\n",
    "               return image1, label, image2\n",
    "        else:\n",
    "           if self.return_img_name:\n",
    "               return image1, label, img_name\n",
    "           else:\n",
    "               return image1, label\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    '''\n",
    "    Focal loss function for imbalanced datasets with multiple classes\n",
    "    '''\n",
    "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        if not self.logits:\n",
    "            predictions = torch.nn.functional.softmax(predictions, dim=-1)\n",
    "\n",
    "        CE_loss = torch.nn.functional.cross_entropy(predictions, targets, reduction='none')\n",
    "        pt = torch.exp(-CE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * CE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTrainer:\n",
    "    def __init__(self, model,epochs, criterion, optimizer,\n",
    "                 train_dataloader,val_dataloader,model_save_dir,save_best_del_rest=True,use_scheduler=False, displ_print=True, pbar_visible=False):\n",
    "        # attributes\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.epochs = epochs\n",
    "        self.val_data_loader = val_dataloader\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.save_best_del_rest = save_best_del_rest      # model saved after each epoch and deleted if not the best\n",
    "        self.use_scheduler = use_scheduler\n",
    "        self.displ_print = displ_print\n",
    "        self.pbar_visible = pbar_visible\n",
    "\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        if use_scheduler:\n",
    "            self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "        # send model to device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # check if model directory exits, otherwise create one\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "\n",
    "        if self.save_best_del_rest:\n",
    "            self.metrics = {'train_loss':{}, 'train_acc':{}, 'val_loss':{}, 'val_acc':{}}\n",
    "\n",
    "    def train(self):\n",
    "        # put model in train mode\n",
    "        self.model.train()\n",
    "\n",
    "        with tqdm(range(self.epochs),disable=not self.pbar_visible) as pbar:\n",
    "            for epoch in pbar:\n",
    "                # running sum of loss and accuracy\n",
    "                loss_list = []\n",
    "                acc_list = []\n",
    "                for i, (img, label) in enumerate(self.train_dataloader):\n",
    "                    # set gradient to zero\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # send to device\n",
    "                    img = img.to(self.device)       # [batch_size, 3, 256, 256]\n",
    "                    label = label.to(self.device)   # [batch_size,]\n",
    "\n",
    "                    # forward pass\n",
    "                    output = self.model(img)        # output: [batch_size, 3]\n",
    "                    \n",
    "                    loss = self.criterion(output, label)\n",
    "\n",
    "                    # backward pass\n",
    "                    loss.backward()\n",
    "\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    # calculate and add \n",
    "                    acc = accuracy_score(label.detach().cpu().numpy(),\n",
    "                                          torch.argmax(torch.softmax(output,dim=-1),dim=-1).detach().cpu().numpy())\n",
    "                    \n",
    "                    # append loss and accuracy\n",
    "                    loss_list.append(loss.item())\n",
    "                    acc_list.append(acc)\n",
    "                \n",
    "                # calculate validation loss and accuracy\n",
    "                val_loss, val_acc = self.validate()\n",
    "\n",
    "                # calculate train loss and accuracy\n",
    "                loss_list = torch.tensor(loss_list)\n",
    "                acc_list = torch.tensor(acc_list)\n",
    "                train_loss = torch.mean(loss_list)\n",
    "                train_acc = torch.mean(acc_list)\n",
    "\n",
    "                # take schedular step\n",
    "                if self.use_scheduler:\n",
    "                    self.lr_scheduler.step()\n",
    "\n",
    "                if self.displ_print:\n",
    "                    print(f'Epoch {epoch} Train Loss: {train_loss} Train Accuracy: {train_acc}, Val Loss: {val_loss}, Val Accuracy: {val_acc}')\n",
    "                pbar.set_description(f'Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "                pbar.update(1)\n",
    "\n",
    "                # save metrics\n",
    "                if self.save_best_del_rest:\n",
    "                    self.metrics['train_loss'][epoch] = train_loss.item()\n",
    "                    self.metrics['train_acc'][epoch] = train_acc.item()\n",
    "                    self.metrics['val_loss'][epoch] = val_loss.item()\n",
    "                    self.metrics['val_acc'][epoch] = val_acc\n",
    "                    torch.save(self.model, os.path.join(self.model_save_dir, f'model_{epoch}.pt'))\n",
    "\n",
    "        # save model\n",
    "        if self.save_best_del_rest:\n",
    "            self.keep_best_delete_rest()\n",
    "        else:\n",
    "            torch.save(self.model, os.path.join(self.model_save_dir, 'model_last_ep.pt'))\n",
    "            with open(os.path.join(self.model_save_dir, 'metrics_saved_model.json'), 'w') as f:\n",
    "                metrics = {'train_loss':train_loss.item(), 'train_acc': train_acc.item(), 'val_loss': val_loss.item(), 'val_acc': val_acc}\n",
    "                json.dump(metrics, f,indent=2)\n",
    "\n",
    "    \n",
    "    def validate(self):\n",
    "\n",
    "        # put model in eval mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # keep list of true and predicted labels\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.val_data_loader):\n",
    "                # send image to device\n",
    "                img = img.to(self.device)    # [batch_size, 3, 256, 256]\n",
    "                label = label.to(self.device)   # [batch_size,]\n",
    "\n",
    "                # forward pass\n",
    "                output = self.model(img)    # [batch_size, 1]\n",
    "\n",
    "                # y_true\n",
    "                y_true.extend(label.cpu().numpy().tolist())\n",
    "\n",
    "                # y_pred\n",
    "                # y_pred.extend(torch.argmax(torch.softmax(output,dim=-1),dim=-1).detach().cpu().numpy())\n",
    "                y_pred.extend(torch.argmax(output,dim=-1).detach().cpu().numpy())\n",
    "\n",
    "                # calculate loss\n",
    "                loss_ = self.criterion(output, label).item()\n",
    "\n",
    "                loss_list.append(loss_)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_list = torch.tensor(loss_list)\n",
    "        loss = torch.mean(loss_list)\n",
    "\n",
    "        # calculate accuracy\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        # put model back in train mode\n",
    "        self.model.train()\n",
    "\n",
    "        return loss, acc \n",
    "    \n",
    "    def keep_best_delete_rest(self):\n",
    "        # sort the models based on validation accuracy\n",
    "        best_val_acc_epoch = max(self.metrics['val_acc'], key=self.metrics['val_acc'].get)\n",
    "\n",
    "        # best model path\n",
    "        best_model_path = os.path.join(self.model_save_dir, f'model_{best_val_acc_epoch}.pt')\n",
    "\n",
    "        for model_path in os.listdir(self.model_save_dir):\n",
    "            if model_path != f'model_{best_val_acc_epoch}.pt':\n",
    "                os.remove(os.path.join(self.model_save_dir, model_path))\n",
    "        \n",
    "        # rename the best model\n",
    "        os.rename(best_model_path, os.path.join(self.model_save_dir, 'model_best.pt'))\n",
    "\n",
    "        # save metrics\n",
    "        with open(os.path.join(self.model_save_dir, 'metrics.json'), 'w') as f:\n",
    "            json.dump(self.metrics, f,indent=2)\n",
    "        \n",
    "        with open(os.path.join(self.model_save_dir, 'metrics_saved_model.json'), 'w') as f:\n",
    "                metrics = {'train_loss':self.metrics['train_loss'][best_val_acc_epoch],\n",
    "                            'train_acc': self.metrics['train_acc'][best_val_acc_epoch],\n",
    "                            'val_loss': self.metrics['val_loss'][best_val_acc_epoch],\n",
    "                            'val_acc': self.metrics['val_acc'][best_val_acc_epoch]}\n",
    "                json.dump(metrics, f,indent=2)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 624\n",
      "Validation dataset size: 157\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the preprocessing transforms\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #grayscale to RGB since the model was trained on RGB images\n",
    "    Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),  # Convert grayscale to RGB\n",
    "    # Normalize the image using the mean and standard deviation of the ImageNet dataset\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Load the dataframe\n",
    "df_path = os.path.join('..','Dataset_BUSI_with_GT','dataset.csv')\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the training and validation datasets\n",
    "train_dataset = UltrasoundDataset(train_df, transform=transforms)\n",
    "val_dataset = UltrasoundDataset(val_df, transform=transforms)\n",
    "\n",
    "# Create the training and validation dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#print train and validation dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class labels values count\n",
      "0  - Normal\n",
      "1  - Benign\n",
      "2  - Malignant\n",
      "true_label_num\n",
      "1    0.560819\n",
      "2    0.268886\n",
      "0    0.170294\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Original class labels values count\")\n",
    "print(0, ' - Normal')\n",
    "print(1, ' - Benign')\n",
    "print(2, ' - Malignant')\n",
    "print(df['true_label_num'].value_counts(normalize=True))\n",
    "\n",
    "df['true_label_num'].value_counts(normalize=True).plot(kind='bar', title='Original class labels values count',xlabel='Class', ylabel='Fraction')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "resnet50 = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    resnet50,\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(1000, 3)\n",
    ")\n",
    "\n",
    "# # freeze resnet layers\n",
    "# for param in model[0].parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # unfreeze last layer and layer before last\n",
    "# for param in model[0].fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "# for param in model[0].layer4.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "MODEL_SAVE_DIR = 'Model_and_metrics'\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            [{'params': model[0].parameters(),'lr': 9e-5},\n",
    "            #  {'params': model[0].layer4.parameters(),'lr': 9e-5},\n",
    "             {'params': model[1].parameters(),'lr': 9e-4}],weight_decay=1e-2)\n",
    "\n",
    "trainer = ClassificationTrainer(\n",
    "    model=model, epochs=10, criterion=criterion, optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader, val_dataloader=val_dataloader,\n",
    "    model_save_dir=MODEL_SAVE_DIR, save_best_del_rest=True, use_scheduler= True ,pbar_visible=False, displ_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# SEED = 42\n",
    "\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "# resnet50 = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     resnet50,\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Dropout(0.5),\n",
    "#     torch.nn.Linear(1000, 3)\n",
    "# )\n",
    "\n",
    "# # # freeze resnet layers\n",
    "# # for param in model[0].parameters():\n",
    "# #     param.requires_grad = False\n",
    "\n",
    "# # # unfreeze last layer and layer before last\n",
    "# # for param in model[0].fc.parameters():\n",
    "# #     param.requires_grad = True\n",
    "# # for param in model[0].layer4.parameters():\n",
    "# #     param.requires_grad = True\n",
    "\n",
    "# # move model to device\n",
    "# model = model.to(DEVICE)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# MODEL_SAVE_DIR = 'Model_and_metrics'\n",
    "\n",
    "# optimizer = torch.optim.Adam(\n",
    "#             [{'params': model[0].parameters(),'lr': 9e-5},\n",
    "#             #  {'params': model[0].layer4.parameters(),'lr': 9e-5},\n",
    "#              {'params': model[1].parameters(),'lr': 9e-4}],weight_decay=1e-2)\n",
    "\n",
    "# trainer = ClassificationTrainer(\n",
    "#     model=model, epochs=2, criterion=criterion, optimizer=optimizer,\n",
    "#     train_dataloader=train_dataloader, val_dataloader=val_dataloader,\n",
    "#     model_save_dir=MODEL_SAVE_DIR, save_best_del_rest=True, use_scheduler= True ,pbar_visible=False, displ_print=True)\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss: 1.1221511363983154 Train Accuracy: 0.3541666666666667, Val Loss: 1.122428297996521, Val Accuracy: 0.3248407643312102\n",
      "Epoch 1 Train Loss: 1.083731770515442 Train Accuracy: 0.4182692307692308, Val Loss: 1.0883985757827759, Val Accuracy: 0.3821656050955414\n",
      "Epoch 2 Train Loss: 1.0537885427474976 Train Accuracy: 0.4407051282051282, Val Loss: 1.0762999057769775, Val Accuracy: 0.47770700636942676\n",
      "Epoch 3 Train Loss: 1.0105637311935425 Train Accuracy: 0.5144230769230769, Val Loss: 1.0376352071762085, Val Accuracy: 0.49044585987261147\n",
      "Epoch 4 Train Loss: 0.9327803254127502 Train Accuracy: 0.5753205128205128, Val Loss: 0.9532183408737183, Val Accuracy: 0.6050955414012739\n",
      "Epoch 5 Train Loss: 0.7972623109817505 Train Accuracy: 0.6858974358974359, Val Loss: 0.774406909942627, Val Accuracy: 0.7070063694267515\n",
      "Epoch 6 Train Loss: 0.6419278383255005 Train Accuracy: 0.7628205128205128, Val Loss: 0.7446151375770569, Val Accuracy: 0.7388535031847133\n",
      "Epoch 7 Train Loss: 0.5326709747314453 Train Accuracy: 0.8012820512820513, Val Loss: 0.7250882387161255, Val Accuracy: 0.7070063694267515\n",
      "Epoch 8 Train Loss: 0.4068743586540222 Train Accuracy: 0.8493589743589743, Val Loss: 0.635332465171814, Val Accuracy: 0.7388535031847133\n",
      "Epoch 9 Train Loss: 0.2954096496105194 Train Accuracy: 0.9006410256410257, Val Loss: 0.620607316493988, Val Accuracy: 0.7515923566878981\n",
      "Epoch 10 Train Loss: 0.2703859508037567 Train Accuracy: 0.9166666666666666, Val Loss: 0.610988438129425, Val Accuracy: 0.7961783439490446\n",
      "Epoch 11 Train Loss: 0.20200885832309723 Train Accuracy: 0.9375, Val Loss: 0.5592645406723022, Val Accuracy: 0.8280254777070064\n",
      "Epoch 12 Train Loss: 0.1571885198354721 Train Accuracy: 0.9631410256410257, Val Loss: 0.5212768316268921, Val Accuracy: 0.8280254777070064\n",
      "Epoch 13 Train Loss: 0.12853598594665527 Train Accuracy: 0.969551282051282, Val Loss: 0.5944364666938782, Val Accuracy: 0.8152866242038217\n",
      "Epoch 14 Train Loss: 0.11558715999126434 Train Accuracy: 0.9791666666666666, Val Loss: 0.5168295502662659, Val Accuracy: 0.8089171974522293\n",
      "Epoch 15 Train Loss: 0.12124979496002197 Train Accuracy: 0.9727564102564102, Val Loss: 0.525469183921814, Val Accuracy: 0.821656050955414\n",
      "Epoch 16 Train Loss: 0.07871776819229126 Train Accuracy: 0.9855769230769231, Val Loss: 0.590766966342926, Val Accuracy: 0.821656050955414\n",
      "Epoch 17 Train Loss: 0.08453375101089478 Train Accuracy: 0.9935897435897436, Val Loss: 0.5684583783149719, Val Accuracy: 0.8280254777070064\n",
      "Epoch 18 Train Loss: 0.0861629918217659 Train Accuracy: 0.9791666666666666, Val Loss: 0.6004921793937683, Val Accuracy: 0.821656050955414\n",
      "Epoch 19 Train Loss: 0.09762657433748245 Train Accuracy: 0.9839743589743589, Val Loss: 0.5779653191566467, Val Accuracy: 0.821656050955414\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "resnet50 = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    resnet50,\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.7),\n",
    "    torch.nn.Linear(1000, 3)\n",
    ")\n",
    "\n",
    "# # freeze resnet layers\n",
    "# for param in model[0].parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # unfreeze last layer and layer before last\n",
    "# for param in model[0].fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "# for param in model[0].layer4.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0/0.1702, 1.0/0.5608, 1.0/0.2688]).to(DEVICE))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            [{'params': model[0].parameters(),'lr': 9e-5},\n",
    "            #  {'params': model[0].layer4.parameters(),'lr': 9e-5},\n",
    "             {'params': model[3].parameters(),'lr': 9e-4}],weight_decay=5e-1)\n",
    "\n",
    "trainer = ClassificationTrainer(\n",
    "    model=model, epochs=20, criterion=criterion, optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader, val_dataloader=val_dataloader,\n",
    "    model_save_dir='ResNet50_try', use_scheduler= True ,pbar_visible=False, displ_print=True)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('ResNet50_try/model_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.59      0.69        29\n",
      "           1       0.79      0.95      0.86        84\n",
      "           2       0.89      0.73      0.80        44\n",
      "\n",
      "    accuracy                           0.82       157\n",
      "   macro avg       0.84      0.76      0.79       157\n",
      "weighted avg       0.83      0.82      0.82       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print classification report\n",
    "#import classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# put model in eval mode\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# keep list of true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (img, label) in enumerate(val_dataloader):\n",
    "        # send image to device\n",
    "        img = img.to(DEVICE)    # [batch_size, 3, 256, 256]\n",
    "        label = label.to(DEVICE)   # [batch_size,]\n",
    "\n",
    "        # forward pass\n",
    "        output = model(img)    # [batch_size, 1]\n",
    "\n",
    "        # y_true\n",
    "        y_true.extend(label.cpu().numpy().tolist())\n",
    "\n",
    "        # y_pred\n",
    "        y_pred.extend(torch.argmax(output,dim=-1).detach().cpu().numpy())\n",
    "\n",
    "        \n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate mask bounding box around the mask leave some margin\n",
    "#also generate center point of the mask\n",
    "\n",
    "def mask_bounding_box(mask,margin=5):\n",
    "    '''\n",
    "    mask: numpy array of the mask\n",
    "    margin: margin around the bounding box\n",
    "    '''\n",
    "    #convert to grayscale if not\n",
    "    if len(mask.shape) == 3 and mask.shape[2] == 3:\n",
    "        gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    elif len(mask.shape) == 2:\n",
    "        gray = mask\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image format\")\n",
    "    \n",
    "    contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #get bounding box\n",
    "    largest_contour=max(contours,key=cv2.contourArea)\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(largest_contour)\n",
    "    #where x,y are the top left corner of the bounding box\n",
    "\n",
    "    #add margin to the bounding box\n",
    "    x_min=x-margin if x-margin > 0 else 0\n",
    "    y_min=y-margin if y-margin > 0 else 0\n",
    "    x_max=x+w+margin if x+w+margin < mask.shape[1] else mask.shape[1]\n",
    "    y_max=y+h+margin if y+h+margin < mask.shape[0] else mask.shape[0]\n",
    "    #get center point\n",
    "    center_x=x+w//2\n",
    "    center_y=y+h//2\n",
    "    return x_min,y_min,x_max,y_max,center_x,center_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:no value was provided for `target_layer`, thus set to '0.layer4'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_prompt:                         image  x_min  y_min  x_max  y_max  center_x  center_y  \\\n",
      "0     malignant (51)_mask.png     85     57    167    105       126        81   \n",
      "1     malignant (44)_mask.png    117     58    197    126       157        92   \n",
      "2    malignant (194)_mask.png     68    117    141    167       104       142   \n",
      "3     malignant (97)_mask.png     61     58    145    125       103        91   \n",
      "4    malignant (143)_mask.png     63     69    144    165       103       117   \n",
      "..                        ...    ...    ...    ...    ...       ...       ...   \n",
      "152     benign (170)_mask.png    124     88    197    141       160       114   \n",
      "153     benign (231)_mask.png     86     89    161    158       123       123   \n",
      "154     benign (399)_mask.png     65     65    156    135       110       100   \n",
      "155      benign (25)_mask.png     98     57    168    114       133        85   \n",
      "156      normal (22)_mask.png     77     53    167    112       122        82   \n",
      "\n",
      "                                                  path  true_label  \\\n",
      "0    ..\\Dataset_BUSI_with_GT\\malignant\\malignant (5...           2   \n",
      "1    ..\\Dataset_BUSI_with_GT\\malignant\\malignant (4...           2   \n",
      "2    ..\\Dataset_BUSI_with_GT\\malignant\\malignant (1...           2   \n",
      "3    ..\\Dataset_BUSI_with_GT\\malignant\\malignant (9...           2   \n",
      "4    ..\\Dataset_BUSI_with_GT\\malignant\\malignant (1...           2   \n",
      "..                                                 ...         ...   \n",
      "152    ..\\Dataset_BUSI_with_GT\\benign\\benign (170).png           1   \n",
      "153    ..\\Dataset_BUSI_with_GT\\benign\\benign (231).png           1   \n",
      "154    ..\\Dataset_BUSI_with_GT\\benign\\benign (399).png           1   \n",
      "155     ..\\Dataset_BUSI_with_GT\\benign\\benign (25).png           1   \n",
      "156     ..\\Dataset_BUSI_with_GT\\normal\\normal (22).png           0   \n",
      "\n",
      "     predicted_label  \n",
      "0                  1  \n",
      "1                  2  \n",
      "2                  1  \n",
      "3                  2  \n",
      "4                  2  \n",
      "..               ...  \n",
      "152                1  \n",
      "153                1  \n",
      "154                2  \n",
      "155                1  \n",
      "156                0  \n",
      "\n",
      "[157 rows x 10 columns]\n",
      "Prompt saved to: ..\\PromptSAM\\prompt_valid.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchcam.utils import overlay_mask\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def get_prompt_from_cam(model, train_df, val_df, SEED, box_margin=5):\n",
    "\n",
    "    df_prompt=pd.DataFrame(columns=['image','x_min','y_min','x_max','y_max','center_x','center_y','path','true_label','predicted_label'])\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # create folder PromptSAM if it does not exist\n",
    "    if not os.path.exists(os.path.join('..', 'PromptSAM')):\n",
    "        os.makedirs(os.path.join('..', 'PromptSAM'))\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        #grayscale to RGB since the model was trained on RGB images\n",
    "        Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),  # Convert grayscale to RGB\n",
    "        # Normalize the image using the mean and standard deviation of the ImageNet dataset\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    transforms_og = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        #grayscale to RGB since the model was trained on RGB images\n",
    "        Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),  # Convert grayscale to RGB\n",
    "    ])\n",
    "\n",
    "    val_dataset_og = UltrasoundDataset(val_df, transform=transforms, transform_og=transforms_og, return_og=True, return_img_name=True)\n",
    "    val_dataloader_og = DataLoader(val_dataset_og, batch_size=1, shuffle=False)\n",
    "\n",
    "    train_dataset_og = UltrasoundDataset(train_df, transform=transforms, transform_og=transforms_og, return_og=True, return_img_name=True)\n",
    "    train_dataloader_og = DataLoader(train_dataset_og, batch_size=1, shuffle=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with GradCAM(model) as cam_extractor:\n",
    "        for img, label,og_img, img_path in val_dataloader_og:\n",
    "            img = img[:1].to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            out = model(img)\n",
    "\n",
    "            activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "            #fig,ax = plt.subplots(1,2)\n",
    "            # plt.imshow(img[0].permute(1, 2, 0).cpu().numpy()); plt.axis('off'); plt.tight_layout(); plt.show()\n",
    "           # result = overlay_mask(to_pil_image(og_img[0]), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.7)\n",
    "            #result=overlay_mask(to_pil_image(og_img[0]), to_pil_image(torch.stack(activation_map).reshape(7,7), mode='F'), alpha=0.7)\n",
    "            # #Display it\n",
    "            #ax[0].imshow(result); ax[0].axis('off'); \n",
    "            #ax[1].imshow(og_img[0].permute(1, 2, 0).cpu().numpy()); ax[1].axis('off'); plt.show()\n",
    "\n",
    "            #thresholding the activation map to get the mask\n",
    "            threshold = 0.5\n",
    "            activation_map= torch.stack(activation_map)\n",
    "            #print('activation map shape:',activation_map.shape)\n",
    "            activation_map = torch.nn.functional.interpolate(activation_map, size=(224,224), mode='bilinear', align_corners=False)\n",
    "            #thresholding\n",
    "            mask = np.where(activation_map[0].squeeze(0).cpu().numpy() > threshold, 1, 0)\n",
    "            #print('actual label:',label[0].item(),'predicted label:',out.argmax().item())\n",
    "\n",
    "            # Display it\n",
    "            # fig,ax = plt.subplots(1,2)\n",
    "            # ax[0].imshow(mask); ax[0].axis('off');\n",
    "            # ax[1].imshow(og_img[0].permute(1, 2, 0).cpu().numpy()); ax[1].axis('off'); plt.show()\n",
    "\n",
    "\n",
    "            #take the product of the mask and the image\n",
    "            #print shape of mask and image\n",
    "\n",
    "            cropped_img = og_img[0].permute(1, 2, 0).cpu().numpy() * mask[:,:,None]\n",
    "\n",
    "            # Display it\n",
    "            #print(cropped_img.shape)\n",
    "\n",
    "            #plt.imshow(cropped_img); plt.axis('off'); plt.show()\n",
    "\n",
    "            #generate mask bounding box around the mask leave some margin\n",
    "\n",
    "            #also generate center point of the mask\n",
    "            mask=mask.astype(np.uint8)\n",
    "\n",
    "            mask_3d = np.stack([mask]*3,axis=-1)\n",
    "\n",
    "            x_min,y_min,x_max,y_max,center_x,center_y=mask_bounding_box(mask_3d,margin=box_margin)\n",
    "\n",
    "            # #draw bounding box on the cropped image\n",
    "            # bbox_point_img= cropped_img.copy()\n",
    "            # cv2.rectangle(bbox_point_img,(x_min,y_min),(x_max,y_max),(0,255,0),2)\n",
    "            # #draw center point\n",
    "            # cv2.circle(bbox_point_img,(center_x,center_y),5,(0,0,255),-1)\n",
    "\n",
    "            #show mask with bounding box and center point\n",
    "            #plt.imshow(bbox_point_img); plt.axis('off'); plt.show()\n",
    "\n",
    "            #get the image name\n",
    "            img_name=img_path[0].split('\\\\')[-1]\n",
    "            img_name=img_name.split('.')[0]+'_mask.png'\n",
    "            true_label = label[0].item()\n",
    "            predicted_label = out.argmax().item()\n",
    "\n",
    "\n",
    "            #store the bounding box and center point in the dataframe in format image,x_min,y_min,x_max,y_max,center_x,center_y,path,true_label,predicted_label\n",
    "            df_prompt.loc[len(df_prompt)] = [img_name,x_min,y_min,x_max,y_max,center_x,center_y,img_path[0],true_label,predicted_label]\n",
    "\n",
    "    print('df_prompt:',df_prompt)\n",
    "    #sort the dataframe by img_name\n",
    "    df_prompt=df_prompt.sort_values(by='image')\n",
    "    #save df_prompt to csv\n",
    "    csv_path=os.path.join('..', 'PromptSAM','prompt_valid.csv')\n",
    "    df_prompt.to_csv(csv_path,index=False)\n",
    "    print('Prompt saved to:',csv_path)\n",
    "get_prompt_from_cam(model, train_df, val_df, SEED, box_margin=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. do ablation study , if regularization increase, quality of cam increase/decrease\n",
    "2. try different cam and different model\n",
    "3. try ensemble of cams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
